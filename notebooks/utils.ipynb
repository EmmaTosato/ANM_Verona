{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Augmentation check",
   "id": "86c212e78bccb07e"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T12:54:10.995423Z",
     "start_time": "2025-04-30T12:54:10.993618Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_augmentations = 10\n",
    "subset_size = 17\n",
    "\n",
    "# Path to augmented FC maps directory\n",
    "fcmaps_augmented_dir = '/Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented'\n",
    "\n",
    "# List of 173 HCP subjects\n",
    "hcp_list_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/list_HCP.txt'\n",
    "\n",
    "# Output txt report\n",
    "path_final_report = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/final_report.txt\"\n",
    "\n",
    "# Path to CSV file with missing SCA files\n",
    "path_sca_missing = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/missing_SCA_files.csv\"\n",
    "\n",
    "# Summary CSV path of augmentation info\n",
    "csv_aug_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/metadata/aug_tracking.csv'"
   ],
   "id": "7003a8bccafa2a04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HCP List and Subsets",
   "id": "ea5bf2a9b0a7a71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if the method used in augmentation.py works (note: it already has checks inside)",
   "id": "209aa33dae56a7a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read HCP file\n",
    "with open(hcp_list_path, \"r\") as f:\n",
    "    hcp_pool = [line.strip() for line in f.readlines()]\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Generate fixed non-overlapping HCP subsets\n",
    "shuffled_hcp = random.sample(hcp_pool, n_augmentations * subset_size)\n",
    "\n",
    "hcp_subsets = [\n",
    "    shuffled_hcp[i * subset_size : (i + 1) * subset_size]\n",
    "    for i in range(n_augmentations)\n",
    "]"
   ],
   "id": "40e800dc35c89f40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each subset has exactly 17 elements",
   "id": "64269fa076d9f986"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for idx, subset in enumerate(hcp_subsets, start=1):\n",
    "    if len(subset) != subset_size:\n",
    "        print(f\"Subset {idx} has incorrect length: {len(subset)} elements\")\n",
    "    else:\n",
    "        print(f\"Subset {idx} OK ({len(subset)} HCP)\")"
   ],
   "id": "2d82b72b387924ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check which HCPs were left out",
   "id": "a5d1273ef81fe6cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "used_hcp = set(shuffled_hcp)\n",
    "hcp_pool_set = set(hcp_pool)\n",
    "excluded_hcp = hcp_pool_set - used_hcp\n",
    "\n",
    "print(\"List of excluded HCPs:\", sorted(excluded_hcp))"
   ],
   "id": "909e19bed65c29a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for overlaps between subsets",
   "id": "37556af58fff5726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Flatten all subsets into a single list\n",
    "all_hcps = [hcp for subset in hcp_subsets for hcp in subset]\n",
    "\n",
    "# Compare total length vs. number of unique HCPs\n",
    "if len(all_hcps) == len(set(all_hcps)):\n",
    "    print(\"Subsets are disjoint: no overlaps.\")\n",
    "else:\n",
    "    print(\"Warning: there are overlapping HCPs between subsets.\")"
   ],
   "id": "668a91915ff648cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Augmentation Check",
   "id": "a43ab1539ccdd292"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CSV",
   "id": "eadb802ea2df1662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "aug_track = pd.read_csv(csv_aug_path)",
   "id": "7857afb66a313c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "aug_track.iloc[:5, :5]",
   "id": "56e6259075e01d15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that there are exactly 177 unique subjects",
   "id": "39f65666a9894b77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_subjects = aug_track['subject'].unique()\n",
    "print(f\"Number of unique subjects: {len(unique_subjects)}\")"
   ],
   "id": "994a8e8fe51a7fca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each subject has 10 augmentations",
   "id": "fa1d55ebc23e734d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "augment_per_subject = aug_track.groupby(\"subject\")[\"augmentation\"].count()\n",
    "subjects_missing = augment_per_subject[augment_per_subject != 10]\n",
    "\n",
    "if subjects_missing.empty:\n",
    "    print(\"All subjects have exactly 10 augmentations.\")\n",
    "else:\n",
    "    print(\"Some subjects DO NOT have 10 augmentations\")\n",
    "    print(subjects_missing)"
   ],
   "id": "78ec9fb9b87eb1dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each augmentation has exactly 17 HCPs for all patients",
   "id": "35560748506c6cbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Verify that each row in the DataFrame has exactly 17 HCPs\n",
    "aug_track[\"hcp_count\"] = aug_track[\"hcp_subset\"].apply(lambda x: len(x.split(\",\")))\n",
    "\n",
    "# Filter rows with fewer or more than 17 HCPs\n",
    "invalid_hcp_counts = aug_track[aug_track[\"hcp_count\"] != 17]\n",
    "\n",
    "# Output\n",
    "if invalid_hcp_counts.empty:\n",
    "    print(\"All augmentations have exactly 17 HCPs.\")\n",
    "else:\n",
    "    print(\"Some augmentations do NOT have 17 HCPs\")"
   ],
   "id": "b66f81fc3bd677cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Excluding inconsistent subjects, check that valid ones use the same HCPs per augmentation index across all subjects.\n",
    "That is:\n",
    "- **Sub1**\n",
    "    - *aug 1*: hcp1, hcp2, hcp3\n",
    "    - *aug 2*: hcp4, hcp5, hcp6\n",
    "    -  ...\n",
    "...\n",
    "- **Sub2**\n",
    "    - *aug 1*: hcp1, hcp2, hcp3\n",
    "    - *aug 2*: hcp4, hcp5, hcp6\n",
    "    -  ..."
   ],
   "id": "eca0fe8cd4be52c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Exclude subjects with incomplete augmentations\n",
    "valid_aug_track = aug_track[aug_track[\"hcp_count\"] == 17].copy()\n",
    "\n",
    "# For each augmentation index, compute the most frequent HCP group\n",
    "ref_hcp_by_aug = valid_aug_track.groupby(\"augmentation\")[\"hcp_subset\"].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# Assign expected value to each row\n",
    "valid_aug_track[\"expected_hcp_subset\"] = valid_aug_track[\"augmentation\"].map(ref_hcp_by_aug)\n",
    "\n",
    "# Find inconsistent subjects\n",
    "df_inconsistent = valid_aug_track[valid_aug_track[\"hcp_subset\"] != valid_aug_track[\"expected_hcp_subset\"]]\n",
    "\n",
    "# Final output\n",
    "if df_inconsistent.empty:\n",
    "    print(\"Same HCPs used for each augmentation across all VALID subjects.\")\n",
    "else:\n",
    "    print(\"Some augmentations have different HCPs between subjects (only among valid ones):\")\n",
    "    display(df_inconsistent[[\"subject\", \"augmentation\", \"hcp_subset\", \"expected_hcp_subset\"]].sort_values([\"augmentation\", \"subject\"]))\n"
   ],
   "id": "fda58ef4c05a2536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Subjects with inconsistencies",
   "id": "171bfd71f8fc9e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Subjects with inconsistencies in HCP subset:\")\n",
    "print(invalid_hcp_counts[\"subject\"].unique())"
   ],
   "id": "e7f6bf6aec1589d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example: inspect one subject with inconsistencies",
   "id": "10fbf3eec8cb9d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter rows for a specific subject\n",
    "filtered = aug_track[aug_track[\"subject\"] == \"4_S_5005\"]"
   ],
   "id": "40a0dacc2a4d65d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "filtered[[\"subject\", \"augmentation\", \"missing_hcps\"]].sort_values(\"augmentation\").reset_index()",
   "id": "54dc4ce1270b60ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Folder",
   "id": "7d36e37a93085602"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if augmentations were actually created",
   "id": "da379ee499fdc3a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List all subfolders\n",
    "subfolders = [f.path for f in os.scandir(fcmaps_augmented_dir) if f.is_dir()]\n",
    "print(f\"Total subfolders (Patients): {len(subfolders)}\\n\")\n",
    "\n",
    "count_problems = 0\n",
    "# Check each subfolder\n",
    "for folder in subfolders:\n",
    "    # List only files (ignore subdirectories)\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    num_files = len(files)\n",
    "\n",
    "    # Only print if not exactly 10\n",
    "    if num_files != 10:\n",
    "        print(f\"[WARNING] {folder} --> {num_files} files (expected 10)\")\n",
    "        count_problems += 1\n",
    "\n",
    "if count_problems == 0:\n",
    "    print(\"All folders contain exactly 10 files.\")\n",
    "else:\n",
    "    print(f\"Total folders with problems: {count_problems}\")"
   ],
   "id": "e78c57ae1e796432"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each subject has 10 unique augmentations",
   "id": "fa6bf15dc4c0e274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "problems = []\n",
    "\n",
    "# Loop over subject folders\n",
    "for subject_dir in os.listdir(fcmaps_augmented_dir):\n",
    "    subject_path = os.path.join(fcmaps_augmented_dir, subject_dir)\n",
    "    if os.path.isdir(subject_path):\n",
    "        files = [f for f in os.listdir(subject_path) if f.endswith(\".nii.gz\")]\n",
    "\n",
    "        # Extract augmentation indices from filenames\n",
    "        found_aug = sorted([\n",
    "            int(f.split(\"aug\")[-1].split(\".\")[0])\n",
    "            for f in files\n",
    "            if \"aug\" in f and f.split(\"aug\")[-1].split(\".\")[0].isdigit()\n",
    "        ])\n",
    "\n",
    "        expected = list(range(1, 11))\n",
    "        if found_aug != expected:\n",
    "            problems.append((subject_dir, found_aug))\n",
    "\n",
    "# Final report\n",
    "if problems:\n",
    "    print(\"Problems found in the following subjects:\")\n",
    "    for subj, augs in problems:\n",
    "        print(f\"[PROBLEM] {subj} has augmentations: {augs}\")\n",
    "else:\n",
    "    print(\"All folders contain augmentations from 1 to 10 correctly.\")\n"
   ],
   "id": "99581328dd2b7734"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing HCPs and SCA files missing",
   "id": "bc4376f8eda88c38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HCP missing as SCA files in the dataset (Lorenzo folder) and here",
   "id": "1a685d6ade8ee691"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSVs\n",
    "sca_missing_df = pd.read_csv(path_sca_missing)\n",
    "aug_track = pd.read_csv(csv_aug_path)\n",
    "\n",
    "# Variables\n",
    "n_total_hcp = 173\n",
    "results = []\n",
    "unexpected_missing_total = set()\n",
    "\n",
    "# Loop through subjects\n",
    "for subject in sca_missing_df[\"subject\"].unique():\n",
    "    # From missing_SCA_files\n",
    "    row_sca = sca_missing_df[sca_missing_df[\"subject\"] == subject]\n",
    "    sca_missing = set(row_sca.iloc[0][\"SCA_files_missing\"].split(\",\"))\n",
    "    count_missing = len(sca_missing)\n",
    "\n",
    "    # From aug_tracking\n",
    "    sub_aug = aug_track[aug_track[\"subject\"] == subject]\n",
    "    expected_hcp = set()\n",
    "    for row in sub_aug[\"missing_hcps\"]:\n",
    "        hcp_ids = row.strip().strip('\"').split(\",\")\n",
    "        expected_hcp.update(hcp_ids)\n",
    "\n",
    "    # Intersections\n",
    "    expected_and_missing = sca_missing.intersection(expected_hcp)\n",
    "    unexpected_and_missing = sca_missing - expected_hcp\n",
    "    unexpected_missing_total.update(unexpected_and_missing)\n",
    "\n",
    "    # Subject output\n",
    "    results.append(f\"Soggetto {subject}:\\n\")\n",
    "    results.append(f\"- Missing SCA files: {count_missing}\\n\")\n",
    "    results.append(f\"- Used SCA files:  {n_total_hcp - count_missing}\\n\")\n",
    "    results.append(f\"- File attesi nell'augmentation e mancanti come SCA: {', '.join(sorted(expected_and_missing))}\\n\")\n",
    "    results.append(f\"- File NON attesi nell'augmentation e mancanti come SCA: {', '.join(sorted(unexpected_and_missing))}\\n\\n\")\n",
    "\n",
    "# Final section\n",
    "results.append(\"Unique list of NON expected SCA-missing files:\\n\")\n",
    "results.append(f\"{', '.join(sorted(unexpected_missing_total))}\\n\")\n",
    "\n",
    "if unexpected_missing_total == excluded_hcp:\n",
    "    results.append(f\"----> The NON expected and missing SCA files match those excluded from augmentation.\\n\")\n",
    "\n",
    "# Write report to file\n",
    "with open(path_final_report, \"w\") as f:\n",
    "    f.writelines(results)\n"
   ],
   "id": "ccb3ab6b41d6615e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
