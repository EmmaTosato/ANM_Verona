{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11601721,"sourceType":"datasetVersion","datasetId":7276265},{"sourceId":11602172,"sourceType":"datasetVersion","datasetId":7276616},{"sourceId":11602206,"sourceType":"datasetVersion","datasetId":7276639}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Utils","metadata":{"_uuid":"c21aef98-f301-4214-bd0b-540feaf38d6b","_cell_guid":"00dc8301-dd14-40c7-8363-157609f4c619","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport nibabel as nib\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.model_selection import StratifiedKFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"3f91a126-9b8f-4ff7-8674-8406452ae508","_cell_guid":"a4ff4eb8-4f1f-4fd7-8811-f46e50371cdf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-15T14:00:52.150138Z","iopub.execute_input":"2025-04-15T14:00:52.150424Z","iopub.status.idle":"2025-04-15T14:00:52.155358Z","shell.execute_reply.started":"2025-04-15T14:00:52.150401Z","shell.execute_reply":"2025-04-15T14:00:52.154563Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\ndata_dir = '/kaggle/input/fc-mean-maps3d'\nlabels_csv = '/kaggle/input/fc-mean-maps3d/labels.csv'\n\n# Variables\ndf_labels = pd.read_csv(labels_csv)","metadata":{"_uuid":"222a7806-2af8-4adc-9b18-05f8436f94ab","_cell_guid":"d594d3ed-d3dc-4355-bd27-152aa667ee39","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-15T13:49:23.589402Z","iopub.execute_input":"2025-04-15T13:49:23.589997Z","iopub.status.idle":"2025-04-15T13:49:23.608808Z","shell.execute_reply.started":"2025-04-15T13:49:23.589972Z","shell.execute_reply":"2025-04-15T13:49:23.608143Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parameters\nbatch_size = 4\nepochs = 10\nlr = 1e-3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlabel = 'Group'            \ntask = 'classification'               # or 'regression'\n\nif task == 'classification':\n    n_classes = len(df_labels[label].unique())\n    criterion = nn.CrossEntropyLoss()\nelse:\n    n_classes = 1\n    criterion = nn.MSELoss()","metadata":{"_uuid":"29299a26-4f4a-4f56-b854-79726f19a9f0","_cell_guid":"8857e190-46df-4d1b-af31-54e64f9f18f4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-15T13:49:24.746189Z","iopub.execute_input":"2025-04-15T13:49:24.746899Z","iopub.status.idle":"2025-04-15T13:49:24.758259Z","shell.execute_reply.started":"2025-04-15T13:49:24.746875Z","shell.execute_reply":"2025-04-15T13:49:24.757724Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"_uuid":"71b69baf-4a66-4844-8ad3-2aafff098638","_cell_guid":"c50b3090-02f5-495b-b43c-dd08b409549c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class FCDataset(Dataset):\n    def __init__(self, data_dir, df_labels, label_column, task, transform=None):\n        assert task in ['classification', 'regression'], \"Task must be 'classification' or 'regression'\" \n        \n        self.data_dir = data_dir\n        self.df_labels = df_labels.reset_index(drop=True)\n        self.label_column = label_column\n        self.task = task\n        self.transform = transform\n\n        if self.task == 'classification':\n            # Convert string labels to class indices\n            unique_labels = sorted(self.df_labels[self.label_column].unique())\n            self.label_mapping = {label: i for i, label in enumerate(unique_labels)}\n\n        self.samples = []\n\n        for _, row in self.df_labels.iterrows():\n            subj_id = row['ID']\n\n            # Classification: map the labels\n            if self.task == 'classification':\n                label = self.label_mapping[row[self.label_column]]\n            \n            # Regression: convert to float\n            else:  \n                label = float(row[self.label_column])\n\n            # Reconstruct the file paths \n            file_path = os.path.join(data_dir, f\"{subj_id}.npy\")\n\n            if os.path.exists(file_path):\n                self.samples.append((file_path, label))\n            else:\n                print(f\"Missing file: {file_path}\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        file_path, label = self.samples[idx]\n\n        # Load and reshape the volume: (1, 91, 109, 91)\n        volume = np.load(file_path)\n        volume = np.expand_dims(volume, axis=0)  \n\n        # Covert volume into a tensor\n        x = torch.tensor(volume, dtype=torch.float32)\n\n        # Convert the label into a tensor:\n        if self.task == 'classification':\n            y = torch.tensor(label, dtype=torch.long)\n        else: \n            y = torch.tensor(label, dtype=torch.float32)\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, y","metadata":{"_uuid":"282010b5-c261-4f26-b39e-b5e445a385e9","_cell_guid":"76ee44b4-bfaf-461c-9d2d-e359a1c3dd9c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-15T13:49:40.765496Z","iopub.execute_input":"2025-04-15T13:49:40.766054Z","iopub.status.idle":"2025-04-15T13:49:40.773548Z","shell.execute_reply.started":"2025-04-15T13:49:40.766033Z","shell.execute_reply":"2025-04-15T13:49:40.772773Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AugmentedFCDataset(Dataset):\n    def __init__(self, data_dir, df_labels, label_column, task, transform=None):\n        assert task in ['classification', 'regression'], \"Task must be 'classification' or 'regression'\"\n        \n        self.data_dir = data_dir\n        self.df_labels = df_labels.reset_index(drop=True)\n        self.label_column = label_column\n        self.task = task\n        self.transform = transform\n\n        if self.task == 'classification':\n            unique_labels = sorted(self.df_labels[self.label_column].unique())\n            self.label_mapping = {label: i for i, label in enumerate(unique_labels)}\n\n        self.samples = []\n\n        for _, row in self.df_labels.iterrows():\n            subj_id = row['ID']\n\n            if self.task == 'classification':\n                label = self.label_mapping[row[self.label_column]]\n            else:\n                label = float(row[self.label_column])\n\n            subject_folder = os.path.join(data_dir, subj_id)\n            if os.path.isdir(subject_folder):\n                for file in os.listdir(subject_folder):\n                    if file.endswith('.npy'):\n                        file_path = os.path.join(subject_folder, file)\n                        self.samples.append((file_path, label))\n            else:\n                print(f\"Warning: missing augmented folder for subject {subj_id}\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        file_path, label = self.samples[idx]\n\n        volume = np.load(file_path)\n        volume = np.expand_dims(volume, axis=0)\n\n        x = torch.tensor(volume, dtype=torch.float32)\n\n        if self.task == 'classification':\n            y = torch.tensor(label, dtype=torch.long)\n        else:\n            y = torch.tensor(label, dtype=torch.float32)\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Models","metadata":{"_uuid":"608536e6-0622-4300-a0c6-bacbcd3477e2","_cell_guid":"e5073b85-a9e0-4e72-adca-97c1938acf9c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Network","metadata":{"_uuid":"599c6358-5e0c-4e8d-8951-8e7f525191dd","_cell_guid":"877db037-f429-48dd-b926-54fef2e59056","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from torchvision.models.video import r3d_18\n\nclass SimpleCNN3D(nn.Module):\n    def __init__(self, n_classes):\n        super(SimpleCNN3D, self).__init__()\n        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm3d(16)\n        self.pool1 = nn.MaxPool3d(2)\n\n        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm3d(32)\n        self.pool2 = nn.MaxPool3d(2)\n\n        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm3d(64)\n        self.pool3 = nn.AdaptiveAvgPool3d(1)\n\n        self.dropout = nn.Dropout(p=0.4)\n        self.fc = nn.Linear(64, n_classes)\n\n    def forward(self, x):\n        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        return self.fc(x)\n\nclass ResNet3D(nn.Module):\n    def __init__(self, n_classes):\n        super(ResNet3D, self).__init__()\n        self.model = r3d_18(pretrained=False)\n        self.model.stem[0] = nn.Conv3d(1, 64, kernel_size=(3,7,7), stride=(1,2,2), padding=(1,3,3), bias=False)\n        self.model.fc = nn.Linear(self.model.fc.in_features, n_classes)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"_uuid":"d965d9da-f3fc-4fd7-a598-95f464f6b0a5","_cell_guid":"0b5186ab-3811-4c8c-844c-b6caff1710b7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-15T13:49:43.168551Z","iopub.execute_input":"2025-04-15T13:49:43.169214Z","iopub.status.idle":"2025-04-15T13:49:46.147110Z","shell.execute_reply.started":"2025-04-15T13:49:43.169188Z","shell.execute_reply":"2025-04-15T13:49:46.146374Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and testing loops","metadata":{"_uuid":"d619b5bd-ef24-411b-a6a8-5167799c90b0","_cell_guid":"6f81d729-eb90-424c-a83b-6d32a8faed32","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device, task):\n    train_losses, val_losses = [], []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n\n        for x_batch, y_batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(x_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * x_batch.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_loss)\n\n        model.eval()\n        val_running_loss = 0\n        all_preds, all_targets = [], []\n\n        with torch.no_grad():\n            for x_val, y_val in val_loader:\n                x_val, y_val = x_val.to(device), y_val.to(device)\n                outputs = model(x_val)\n                loss = criterion(outputs, y_val)\n                val_running_loss += loss.item() * x_val.size(0)\n\n                preds = torch.argmax(outputs, dim=1) if task == 'classification' else outputs.squeeze()\n\n                all_preds.extend(preds.cpu().numpy())\n                all_targets.extend(y_val.cpu().numpy())\n\n        val_loss = val_running_loss / len(val_loader.dataset)\n        val_losses.append(val_loss)\n\n        if task == 'classification':\n            acc = accuracy_score(all_targets, all_preds)\n            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {acc:.4f}\")\n        else:\n            r2 = r2_score(all_targets, all_preds)\n            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f} - Val R²: {r2:.4f}\")\n\n    return model, train_losses, val_losses","metadata":{"_uuid":"115e837f-880b-4b05-9723-920f40b0435e","_cell_guid":"4f65cdf2-80b2-4a90-ad26-fb653985c3f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_loader, task, device):\n    model.eval()\n    true_labels, pred_labels = [], []\n\n    with torch.no_grad():\n        for x, y in test_loader:\n            x, y = x.to(device), y.to(device)\n            outputs = model(x)\n\n            if task == 'classification':\n                preds = torch.argmax(outputs, dim=1)\n            else:\n                preds = outputs.squeeze()\n\n            true_labels.extend(y.cpu().numpy())\n            pred_labels.extend(preds.cpu().numpy())\n\n    return np.array(true_labels), np.array(pred_labels)","metadata":{"_uuid":"11d5f51c-8f0d-441f-923e-aa650a58c79f","_cell_guid":"2b7943a4-1f81-4cb2-87cb-95983e6d76d2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T14:39:21.492892Z","iopub.execute_input":"2025-04-15T14:39:21.493432Z","iopub.status.idle":"2025-04-15T14:39:21.572282Z","shell.execute_reply.started":"2025-04-15T14:39:21.493411Z","shell.execute_reply":"2025-04-15T14:39:21.571723Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot","metadata":{"_uuid":"37d2c63f-dc6d-46db-a9dd-f53218a2a1aa","_cell_guid":"a7b85f22-cfdd-4462-872a-af4c08ad78bc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def plot_losses(train_losses, val_losses):\n    plt.figure(figsize=(10, 4))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"_uuid":"aaff6253-1cc1-4a1b-9026-d816cf63efbf","_cell_guid":"35ff6539-c1d5-44e8-a3bd-0196dd5ba87b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{"_uuid":"1764650c-ddc3-4653-8a24-237dc05978ae","_cell_guid":"29481649-4e5b-44a2-bf8e-799170df0016","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Pairwise classification","metadata":{}},{"cell_type":"code","source":"def prepare_pair(df_labels, group1, group2):\n    df_pair = df_labels[df_labels['Group'].isin([group1, group2])].reset_index(drop=True)\n    label_mapping = {group1: 0, group2: 1}\n    df_pair['label'] = df_pair['Group'].map(label_mapping)\n    return df_pair","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pair = prepare_pair(df_labels, \"AD\", \"CBS\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting","metadata":{}},{"cell_type":"code","source":"train_df, test_df = train_test_split(\n    df_pair,\n    test_size=0.2,\n    stratify=df_pair['label'],\n    random_state=42\n)","metadata":{"_uuid":"be3ea023-4275-4353-8a0f-0c06c104cccc","_cell_guid":"f746664b-8d09-4903-ac4c-2672f98617e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"f787bdd2-c41f-47f5-ba3c-448c83c10070","_cell_guid":"c16de36e-1feb-493a-8c80-eff8ce02da0d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nsubjects = train_df['ID'].values\nlabels = train_df['label'].values\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(subjects, labels)):\n    df_train_fold = train_df[train_df['ID'].isin(subjects[train_idx])]\n    df_val_fold   = train_df[train_df['ID'].isin(subjects[val_idx])]\n\n    # Create datasets\n    train_dataset = AugmentedFCDataset(path_aug, df_train_fold, 'label', 'classification')\n    val_dataset   = FCDataset(path_nonaug, df_val_fold, 'label', 'classification')\n\n    # Create DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    model = SimpleCNN3D(n_classes=2).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    model, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device, task)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{"_uuid":"c811932d-8d08-45f9-94b4-106e5eba413b","_cell_guid":"b40f7ecf-5b6c-4c9d-ae5a-66445c971220","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Test\ny_true, y_pred = evaluate_model(model, test_loader, task, device)","metadata":{"_uuid":"1195abff-d6f8-4548-b49b-92ca3c1081e4","_cell_guid":"870b006a-9748-49e7-ab07-6f206ca3f70d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification","metadata":{"_uuid":"c83193a0-2616-4cb1-85e8-59de72b79a71","_cell_guid":"6a7c3a96-9750-46fd-97b1-08c7621130cd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Metrics\nprint(\"\\nTest Classification Report:\")\nreport = classification_report(true_labels, pred_labels, output_dict=True, zero_division=0)\ndf_report = pd.DataFrame(report).transpose()\nprint(df_report.round(3))","metadata":{"_uuid":"9ec2e14c-7605-4b84-8230-8e4bdb2c930b","_cell_guid":"3ec12b91-8b68-45ea-8d44-64ffff055f2e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-15T14:39:26.990626Z","iopub.execute_input":"2025-04-15T14:39:26.991098Z","iopub.status.idle":"2025-04-15T14:39:27.004956Z","shell.execute_reply.started":"2025-04-15T14:39:26.991074Z","shell.execute_reply":"2025-04-15T14:39:27.004217Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion matrix\nconf_matrix = confusion_matrix(true_labels, pred_labels)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Test Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","metadata":{"_uuid":"f1b849cd-8345-40eb-88a6-d61006f5258b","_cell_guid":"1ed22330-ec98-44d9-9799-be76aa7d585f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T14:05:50.383161Z","iopub.execute_input":"2025-04-15T14:05:50.383438Z","iopub.status.idle":"2025-04-15T14:05:50.526710Z","shell.execute_reply.started":"2025-04-15T14:05:50.383418Z","shell.execute_reply":"2025-04-15T14:05:50.525891Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### TODO:\n- Multiclass Precision-Recall Curve\n- Plotting the Multiclass ROC Curve","metadata":{"_uuid":"0d250216-739a-4059-ad65-5cec6df22434","_cell_guid":"bc136c2d-720f-4449-860e-198c355e7090","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-15T14:07:21.712122Z","iopub.execute_input":"2025-04-15T14:07:21.712736Z","iopub.status.idle":"2025-04-15T14:07:21.715934Z","shell.execute_reply.started":"2025-04-15T14:07:21.712709Z","shell.execute_reply":"2025-04-15T14:07:21.715077Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Regression","metadata":{"_uuid":"b8b4b2e9-cbe1-40f7-8614-4e91954fc583","_cell_guid":"3cae75c5-fd9a-4c6d-9adc-fa716ce29c41","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"y_true = np.array(true_labels)\ny_pred = np.array(pred_labels)\nresiduals = y_true - y_pred\n\nmse = mean_squared_error(y_true, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_true, y_pred)\nr2 = r2_score(y_true, y_pred)\ntry:\n    msle = mean_squared_log_error(y_true, y_pred)\nexcept ValueError:\n    msle = np.nan\n\nprint(\"\\nTest Regression Metrics:\")\nmetrics = {\n    \"MSE\": mse,\n    \"RMSE\": rmse,\n    \"MAE\": mae,\n    \"R2 Score\": r2,\n    \"MSLE\": msle\n}\ndf_metrics = pd.DataFrame(metrics, index=[\"Value\"]).T.round(4)\ndisplay(df_metrics)\n\n# Scatter plot: Predicted vs Actual\nplt.scatter(y_true, y_pred, alpha=0.6)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Actual vs Predicted\")\nplt.grid(True)\nplt.show()\n\n# Histogram of residuals\nplt.hist(residuals, bins=30, edgecolor='black')\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n# Residuals vs Fitted\nplt.scatter(y_pred, residuals, alpha=0.6)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residuals vs Fitted Values\")\nplt.grid(True)\nplt.show()\n\n# Normality Q-Q Plot\nimport scipy.stats as stats\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title(\"Normal Q-Q Plot\")\nplt.grid(True)\nplt.show()","metadata":{"_uuid":"8b47c5bb-c57d-4e5e-af3d-625ee1fd6416","_cell_guid":"00ece5e4-9330-44b1-9a4e-1431559f728b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}