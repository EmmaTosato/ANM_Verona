{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-28T15:22:43.503001Z",
     "start_time": "2025-04-28T15:22:43.499961Z"
    }
   },
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:22:00.356906Z",
     "start_time": "2025-04-28T15:22:00.352019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_augmentations = 10\n",
    "subset_size = 17\n",
    "\n",
    "# Path per directory augmented\n",
    "fcmaps_augmented_dir = '/Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented'\n",
    "\n",
    "# Lista 173 HCP\n",
    "hcp_list_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/list_HCP.txt'\n",
    "\n",
    "# Txt di output\n",
    "path_missing_report = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/report_hcp_mancanti.txt\"\n",
    "path_final_report = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/final_report.txt\"\n",
    "\n",
    "# Path per il file CSV con missing SCA\n",
    "path_sca_missing = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/missing_SCA_files.csv\"\n",
    "\n",
    "# Path csv riassuntivo delle augmentation\n",
    "csv_aug_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/metadata/aug_tracking.csv'"
   ],
   "id": "fb91f75b6821ae11",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HCP List",
   "id": "a9f3738ce87703b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Controllo se il metodo utilizzato nella funzione augmentation.py funziona (ma anche l√† ci sono dei controlli)",
   "id": "94e31ebbb59e225f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.052092Z",
     "start_time": "2025-04-28T15:20:52.049215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read HCP file\n",
    "with open(hcp_list_path, \"r\") as f:\n",
    "    hcp_pool = [line.strip() for line in f.readlines()]\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Generate fixed HCP subsets that NOT overlap\n",
    "shuffled_hcp = random.sample(hcp_pool, n_augmentations * subset_size)\n",
    "\n",
    "hcp_subsets = [\n",
    "    shuffled_hcp[i * subset_size : (i + 1) * subset_size]\n",
    "    for i in range(n_augmentations)\n",
    "]"
   ],
   "id": "76c25420121151d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Controlla che ogni subset sia di 17",
   "id": "e46ee78fa1b1ee09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.061787Z",
     "start_time": "2025-04-28T15:20:52.058111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for idx, subset in enumerate(hcp_subsets, start=1):\n",
    "    if len(subset) != subset_size:\n",
    "        print(f\"Subset {idx} ha una lunghezza errata: {len(subset)} elementi\")\n",
    "    else:\n",
    "        print(f\"Subset {idx} OK ({len(subset)} HCP)\")"
   ],
   "id": "212bf75cc0b5ae79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1 OK (17 HCP)\n",
      "Subset 2 OK (17 HCP)\n",
      "Subset 3 OK (17 HCP)\n",
      "Subset 4 OK (17 HCP)\n",
      "Subset 5 OK (17 HCP)\n",
      "Subset 6 OK (17 HCP)\n",
      "Subset 7 OK (17 HCP)\n",
      "Subset 8 OK (17 HCP)\n",
      "Subset 9 OK (17 HCP)\n",
      "Subset 10 OK (17 HCP)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Controlla quali sono quelli che rimangono fuori",
   "id": "6d35bd51b6431ef8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.070954Z",
     "start_time": "2025-04-28T15:20:52.068613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "used_hcp = set(shuffled_hcp)\n",
    "hcp_pool_set = set(hcp_pool)\n",
    "excluded_hcp = hcp_pool_set - used_hcp\n",
    "\n",
    "print(\"Lista HCP esclusi:\", sorted(excluded_hcp))"
   ],
   "id": "1d4f7e18ab95a62b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista HCP esclusi: ['146432', '178647', '826353']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Controlla overlap",
   "id": "230f6e8be41db235"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.085838Z",
     "start_time": "2025-04-28T15:20:52.083548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flatten all subsets into a single list\n",
    "all_hcps = [hcp for subset in hcp_subsets for hcp in subset]\n",
    "\n",
    "# Compare total length vs. number of unique HCPs\n",
    "if len(all_hcps) == len(set(all_hcps)):\n",
    "    print(\"Subsets are disjoint: no overlaps.\")\n",
    "else:\n",
    "    print(\"Warning: there are overlapping HCPs between subsets.\")"
   ],
   "id": "cb146bad185ecd64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets are disjoint: no overlaps.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Augmentation Check",
   "id": "381fb2282309431b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CSV",
   "id": "f6c7e4f3a1f2c294"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.103588Z",
     "start_time": "2025-04-28T15:20:52.096072Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": "aug_track = pd.read_csv(csv_aug_path)",
   "id": "a12e018f2f006095"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.125304Z",
     "start_time": "2025-04-28T15:20:52.118612Z"
    }
   },
   "cell_type": "code",
   "source": "aug_track.iloc[:5, :5]",
   "id": "b66a6a8018b6cf0d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      subject  augmentation                                         hcp_subset\n",
       "0  002_S_4654             1  109123,111514,114823,134627,134829,140117,1458...\n",
       "1  002_S_4654             2  102311,158136,159239,169747,177140,191841,2092...\n",
       "2  002_S_4654             3  115825,116726,135124,146735,157336,186949,2002...\n",
       "3  002_S_4654             4  108323,118225,146129,167440,169343,192641,1996...\n",
       "4  002_S_4654             5  115017,131722,132118,150423,167036,169040,1777..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>augmentation</th>\n",
       "      <th>hcp_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002_S_4654</td>\n",
       "      <td>1</td>\n",
       "      <td>109123,111514,114823,134627,134829,140117,1458...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002_S_4654</td>\n",
       "      <td>2</td>\n",
       "      <td>102311,158136,159239,169747,177140,191841,2092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_S_4654</td>\n",
       "      <td>3</td>\n",
       "      <td>115825,116726,135124,146735,157336,186949,2002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002_S_4654</td>\n",
       "      <td>4</td>\n",
       "      <td>108323,118225,146129,167440,169343,192641,1996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002_S_4654</td>\n",
       "      <td>5</td>\n",
       "      <td>115017,131722,132118,150423,167036,169040,1777...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Controllo che ogni soggetto abbiamo 10 augmentation",
   "id": "1896f74f8f9b5a94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.180949Z",
     "start_time": "2025-04-28T15:20:52.174644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "augment_per_subject = aug_track.groupby(\"subject\")[\"augmentation\"].count()\n",
    "subjects_missing = augment_per_subject[augment_per_subject != 10]\n",
    "\n",
    "if subjects_missing.empty:\n",
    "    print(\"Tutti i soggetti hanno esattamente 10 augmentation.\")\n",
    "else:\n",
    "    print(\"Alcuni soggetti NON hanno 10 augmentation:\")\n",
    "    display(subjects_missing)"
   ],
   "id": "26798c9b0403d0cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcuni soggetti NON hanno 10 augmentation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subject\n",
       "002_S_4654    30\n",
       "002_S_6695    30\n",
       "003_S_6264    30\n",
       "Name: augmentation, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Controllo che ogni augmentation abbia lo stesso numero di HCP soggetti per tutti i pazienti",
   "id": "de1d33f5a184a658"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.217650Z",
     "start_time": "2025-04-28T15:20:52.212005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verifica che ogni riga del DataFrame aug_track abbia esattamente 17 HCP\n",
    "aug_track[\"hcp_count\"] = aug_track[\"hcp_subset\"].apply(lambda x: len(x.split(\",\")))\n",
    "\n",
    "# Filtra le righe con meno o pi√π di 17 HCP\n",
    "invalid_hcp_counts = aug_track[aug_track[\"hcp_count\"] != 17]\n",
    "\n",
    "# Output\n",
    "if invalid_hcp_counts.empty:\n",
    "    print(\"Tutte le augmentation hanno esattamente 17 HCP.\")\n",
    "else:\n",
    "    print(\"Alcune augmentation non hanno 17 HCP\")\n",
    "    #display(invalid_hcp_counts[[\"subject\", \"augmentation\", \"hcp_count\"]])"
   ],
   "id": "3f6ba3357337c7f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le augmentation hanno esattamente 17 HCP.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Escludendo i soggetti sopra, controllo che quelli con 17 HCP siano coerenti tra loro, ovvero che per ogni soggetto gli HCP usati nelle varie augmentation siano uguali. Quindi:\n",
    "- **Sub1**\n",
    "    - *aug 1*: hcp1, hcp2, hcp3\n",
    "    - *aug 2*: hcp4, hcp5, hcp6\n",
    "    -  ...\n",
    "...\n",
    "- **Sub2**\n",
    "    - *aug 1*: hcp1, hcp2, hcp3\n",
    "    - *aug 2*: hcp4, hcp5, hcp6\n",
    "    -  ..."
   ],
   "id": "f44b8a3390f8db27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.269700Z",
     "start_time": "2025-04-28T15:20:52.260087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Escludi i soggetti con augmentations incomplete\n",
    "valid_aug_track = aug_track[aug_track[\"hcp_count\"] == 17].copy()\n",
    "\n",
    "# Per ogni augmentation, calcola il gruppo hcp pi√π frequente\n",
    "ref_hcp_by_aug = valid_aug_track.groupby(\"augmentation\")[\"hcp_subset\"].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# Assegna il valore atteso a ogni riga\n",
    "valid_aug_track[\"expected_hcp_subset\"] = valid_aug_track[\"augmentation\"].map(ref_hcp_by_aug)\n",
    "\n",
    "# Trova soggetti incoerenti\n",
    "df_inconsistent = valid_aug_track[valid_aug_track[\"hcp_subset\"] != valid_aug_track[\"expected_hcp_subset\"]]\n",
    "\n",
    "# Output finale\n",
    "if df_inconsistent.empty:\n",
    "    print(\"Gli stessi HCP sono stati usati per ogni augmentation nei soggetti validi.\")\n",
    "else:\n",
    "    print(\"Alcune augmentation hanno gruppi HCP diversi tra soggetti (solo tra quelli validi):\")\n",
    "    display(df_inconsistent[[\"subject\", \"augmentation\", \"hcp_subset\", \"expected_hcp_subset\"]].sort_values([\"augmentation\", \"subject\"]))\n"
   ],
   "id": "f43f1a4fc48766c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gli stessi HCP sono stati usati per ogni augmentation nei soggetti validi.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Soggetti che contengono incorenze",
   "id": "f7315fca5e4ee9ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:20:52.310324Z",
     "start_time": "2025-04-28T15:20:52.306024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Soggetti con inconsistenze negli HCP subset:\")\n",
    "print(invalid_hcp_counts[\"subject\"].unique())"
   ],
   "id": "1d3de7687ec75d72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soggetti con inconsistenze negli HCP subset:\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vedi file per controllare le mancanze",
   "id": "e77985ceab22170f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:03:18.515672Z",
     "start_time": "2025-04-24T16:03:18.500811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Funzione per calcolare gli HCP mancanti\n",
    "def get_missing_hcps(expected, actual):\n",
    "    expected_set = set(expected.split(\",\"))\n",
    "    actual_set = set(actual.split(\",\"))\n",
    "    return sorted(expected_set - actual_set)\n",
    "\n",
    "# Calcola expected_hcp_subset per ogni augmentation (pattern atteso)\n",
    "aug_track[\"expected_hcp_subset\"] = aug_track[\"augmentation\"].map(\n",
    "    aug_track.groupby(\"augmentation\")[\"hcp_subset\"].agg(lambda x: x.mode().iloc[0])\n",
    ")\n",
    "\n",
    "# Filtra righe con augmentations non complete\n",
    "incomplete_rows = aug_track[aug_track[\"hcp_count\"] != 17]\n",
    "\n",
    "# Report\n",
    "with open(path_missing_report, \"w\") as f:\n",
    "    for subject in incomplete_rows[\"subject\"].unique():\n",
    "        f.write(f\"Soggetto {subject}\\n\")\n",
    "        subj_df = incomplete_rows[incomplete_rows[\"subject\"] == subject]\n",
    "        for _, row in subj_df.iterrows():\n",
    "            missing = get_missing_hcps(row[\"expected_hcp_subset\"], row[\"hcp_subset\"])\n",
    "            if missing:\n",
    "                f.write(f\"Augmentation {int(row['augmentation'])}\\n\")\n",
    "                f.write(f\"File HCP mancanti: {', '.join(missing)}\\n\\n\")\n",
    "        f.write(\"-\" * 40 + \"\\n\\n\")"
   ],
   "id": "60a9dcd882ce2348",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Folder",
   "id": "6cdbfa11e1aca02f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Controllo quindi se √® avvenuta o meno l'augmentation",
   "id": "5931eae38da04e84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:25:02.182182Z",
     "start_time": "2025-04-28T15:25:02.158035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List all subfolders\n",
    "subfolders = [f.path for f in os.scandir(fcmaps_augmented_dir) if f.is_dir()]\n",
    "print(f\"Total subfolders: {len(subfolders)}\\n\")\n",
    "\n",
    "# Check each subfolder\n",
    "for folder in subfolders:\n",
    "    # List only files (ignore subdirectories)\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    num_files = len(files)\n",
    "\n",
    "    # Only print if not exactly 10\n",
    "    if num_files != 10:\n",
    "        print(f\"[WARNING] {folder} --> {num_files} files (expected 10)\")"
   ],
   "id": "4cb685ed483f3274",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subfolders: 177\n",
      "\n",
      "[WARNING] /Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented/4_S_5005 --> 0 files (expected 10)\n",
      "[WARNING] /Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented/4_S_5003 --> 0 files (expected 10)\n",
      "[WARNING] /Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented/3_S_5003 --> 0 files (expected 10)\n",
      "[WARNING] /Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented/4_S_5008 --> 0 files (expected 10)\n",
      "[WARNING] /Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented/4_S_5007 --> 0 files (expected 10)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing HCPs and SCA files missing",
   "id": "8120d838ca7349fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HCP missing as SCA files in the dataset (Lorenzo folder) and here",
   "id": "edef6a75978e26d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:54:31.986560Z",
     "start_time": "2025-04-24T16:54:31.971677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica dati da CSV\n",
    "sca_missing_df = pd.read_csv(path_sca_missing)\n",
    "\n",
    "# Funzione per leggere il report delle mancanze HCP\n",
    "with open(path_missing_report, \"r\") as file:\n",
    "    report_lines = file.readlines()\n",
    "\n",
    "results = []\n",
    "current_subject = None\n",
    "all_missing_hcp = set()\n",
    "unexpected_missing_total = set()\n",
    "\n",
    "for line in report_lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"Soggetto\"):\n",
    "        if current_subject:\n",
    "            sca_row = sca_missing_df[sca_missing_df['subject'] == current_subject]\n",
    "            if not sca_row.empty:\n",
    "                sca_missing = set(sca_row.iloc[0]['SCA_files_missing'].split(\",\"))\n",
    "\n",
    "                expected_and_missing = all_missing_hcp.intersection(sca_missing)\n",
    "                unexpected_and_missing = sca_missing - all_missing_hcp\n",
    "\n",
    "                results.append(f\"Soggetto {current_subject}\\n\")\n",
    "                results.append(f\"File attesi e mancanti come SCA: {', '.join(sorted(expected_and_missing))}\\n\")\n",
    "                results.append(f\"File NON attesi e mancanti come SCA: {', '.join(sorted(unexpected_and_missing))}\\n\\n\")\n",
    "\n",
    "                unexpected_missing_total.update(unexpected_and_missing)\n",
    "\n",
    "            all_missing_hcp.clear()\n",
    "\n",
    "        current_subject = line.split()[1]\n",
    "\n",
    "    elif line.startswith(\"File HCP mancanti\"):\n",
    "        missing_hcp = line.replace(\"File HCP mancanti: \", \"\").split(\", \")\n",
    "        all_missing_hcp.update(missing_hcp)\n",
    "\n",
    "# Gestisce l'ultimo soggetto nel ciclo stesso senza bisogno di duplicare codice\n",
    "if current_subject:\n",
    "    sca_row = sca_missing_df[sca_missing_df['subject'] == current_subject]\n",
    "    if not sca_row.empty:\n",
    "        sca_missing = set(sca_row.iloc[0]['SCA_files_missing'].split(\",\"))\n",
    "\n",
    "        expected_and_missing = all_missing_hcp.intersection(sca_missing)\n",
    "        unexpected_and_missing = sca_missing - all_missing_hcp\n",
    "\n",
    "        results.append(f\"Soggetto {current_subject}\\n\")\n",
    "        results.append(f\"File attesi e mancanti come SCA: {', '.join(sorted(expected_and_missing))}\\n\")\n",
    "        results.append(f\"File NON attesi e mancanti come SCA: {', '.join(sorted(unexpected_and_missing))}\\n\\n\")\n",
    "\n",
    "        unexpected_missing_total.update(unexpected_and_missing)\n",
    "\n",
    "# Aggiungi alla fine la lista totale degli inattesi mancanti\n",
    "results.append(\"Lista totale dei file NON attesi e mancanti come SCA:\\n\")\n",
    "results.append(f\"{', '.join(sorted(unexpected_missing_total))}\\n\")\n",
    "\n",
    "# Scrivi risultati\n",
    "with open(path_final_report, \"w\") as file:\n",
    "    file.writelines(results)"
   ],
   "id": "3ba803b7b2e9492c",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:54:46.816968Z",
     "start_time": "2025-04-24T16:54:46.814041Z"
    }
   },
   "cell_type": "code",
   "source": "unexpected_missing_total",
   "id": "c455b200c4cc9b34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'146432', '178647', '826353'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Coincidono con quelli eslcusi dalla divisione pre-augmentation?",
   "id": "29f033e75a0cce4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:59:09.075340Z",
     "start_time": "2025-04-24T16:59:09.072274Z"
    }
   },
   "cell_type": "code",
   "source": "excluded_hcp == unexpected_missing_total",
   "id": "cb3caa8327416b31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
