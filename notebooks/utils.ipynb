{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debug",
   "id": "d414235754662cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T11:07:53.470886Z",
     "start_time": "2025-06-19T11:07:53.465447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Percorso al file di log\n",
    "log_file_path = \"/data/users/etosato/ANM_Verona/src/cnn/runs/run1/log_train1\"\n",
    "\n",
    "# Pattern regex\n",
    "epoch_pattern = re.compile(r\"\\[VAL DEBUG - EPOCH (\\d+)\\]\")\n",
    "debug_pattern = re.compile(\n",
    "    r\"\\[VAL DEBUG\\] ID: ([\\w_]+) \\| True: (\\d) \\| Pred: (\\d) \\| Prob: \\[([\\d.e\\-]+), ([\\d.e\\-]+)\\]\"\n",
    ")\n",
    "\n",
    "# Variabili temporanee\n",
    "current_epoch = None\n",
    "parsed_data = []\n",
    "\n",
    "# Parsing del file\n",
    "with open(log_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        epoch_match = epoch_pattern.match(line)\n",
    "        debug_match = debug_pattern.match(line)\n",
    "\n",
    "        if epoch_match:\n",
    "            current_epoch = int(epoch_match.group(1))\n",
    "        elif debug_match and current_epoch is not None:\n",
    "            subj_id = debug_match.group(1)\n",
    "            true_label = int(debug_match.group(2))\n",
    "            pred_label = int(debug_match.group(3))\n",
    "            prob0 = float(debug_match.group(4))\n",
    "            prob1 = float(debug_match.group(5))\n",
    "\n",
    "            parsed_data.append({\n",
    "                \"epoch\": current_epoch,\n",
    "                \"ID\": subj_id,\n",
    "                \"true\": true_label,\n",
    "                \"pred\": pred_label,\n",
    "                \"prob_0\": prob0,\n",
    "                \"prob_1\": prob1,\n",
    "            })\n",
    "\n",
    "# Creazione DataFrame\n",
    "df_debug = pd.DataFrame(parsed_data)\n",
    "\n",
    "# Conteggio frequenza per soggetto\n",
    "id_counts = df_debug[\"ID\"].value_counts().reset_index()\n",
    "id_counts.columns = [\"ID\", \"count\"]\n"
   ],
   "id": "29e8890d80164e50",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T11:08:55.878418Z",
     "start_time": "2025-06-19T11:08:55.874505Z"
    }
   },
   "cell_type": "code",
   "source": "id_counts",
   "id": "e9b6ffed286fbe5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            ID  count\n",
       "0     1_S_5101      4\n",
       "1     1_S_5093      4\n",
       "2     1_S_5030      4\n",
       "3     1_S_5058      4\n",
       "4     1_S_5074      4\n",
       "..         ...    ...\n",
       "79    1_S_5096      1\n",
       "80    1_S_5062      1\n",
       "81  305_S_6498      1\n",
       "82    1_S_5064      1\n",
       "83    1_S_5067      1\n",
       "\n",
       "[84 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_S_5101</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_S_5093</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_S_5030</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_S_5058</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_S_5074</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1_S_5096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1_S_5062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>305_S_6498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1_S_5064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1_S_5067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows Ã— 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "47bad1b29b09e0f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T22:09:40.201501Z",
     "start_time": "2025-07-13T22:09:39.942647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "base_dir = \"/data/users/etosato/ANM_Verona/src/cnn/tuning_results\"\n",
    "csv_pairs = [\n",
    "    (\"adni_psp.csv\", \"all_testing_results_ADNI_PSP.csv\"),\n",
    "    (\"adni_cbs.csv\", \"all_testing_results_ADNI_CBS.csv\"),\n",
    "    (\"psp_cbs.csv\", \"all_testing_results_PSP_CBS.csv\"),\n",
    "]\n",
    "\n",
    "merged_dfs = []\n",
    "\n",
    "for tuning_file, results_file in csv_pairs:\n",
    "    # Load CSVs\n",
    "    df_tuning = pd.read_csv(f\"{base_dir}/{tuning_file}\")\n",
    "    df_results = pd.read_csv(f\"{base_dir}/{results_file}\")\n",
    "\n",
    "    # Drop duplicate 'group' column from tuning\n",
    "    df_tuning = df_tuning.drop(columns=[\"group\"])\n",
    "\n",
    "    # Merge on ['tuning', 'config']\n",
    "    df_merged = pd.merge(df_results, df_tuning, on=[\"tuning\", \"config\"], how=\"left\")\n",
    "\n",
    "    # Save or collect for further usage\n",
    "    merged_dfs.append(df_merged)\n",
    "\n",
    "    # Optionally: save merged result to new CSV\n",
    "    merged_filename = results_file.replace(\"all_testing_results_\", \"merged_\")\n",
    "    df_merged.to_csv(f\"{base_dir}/{merged_filename}\", index=False)\n"
   ],
   "id": "7990d3588aa9baa6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Augmentation check",
   "id": "86c212e78bccb07e"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T12:54:10.995423Z",
     "start_time": "2025-04-30T12:54:10.993618Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_augmentations = 10\n",
    "subset_size = 17\n",
    "\n",
    "# Path to augmented FC maps directory\n",
    "fcmaps_augmented_dir = '/Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps_augmented'\n",
    "\n",
    "# List of 173 HCP subjects\n",
    "hcp_list_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/list_HCP.txt'\n",
    "\n",
    "# Output txt report\n",
    "path_final_report = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/final_report.txt\"\n",
    "\n",
    "# Path to CSV file with missing SCA files\n",
    "path_sca_missing = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/debugging/missing_SCA_files.csv\"\n",
    "\n",
    "# Summary CSV path of augmentation info\n",
    "csv_aug_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/metadata/aug_tracking.csv'"
   ],
   "id": "7003a8bccafa2a04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HCP List and Subsets",
   "id": "ea5bf2a9b0a7a71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if the method used in augmentation.py works (note: it already has checks inside)",
   "id": "209aa33dae56a7a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read HCP file\n",
    "with open(hcp_list_path, \"r\") as f:\n",
    "    hcp_pool = [line.strip() for line in f.readlines()]\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Generate fixed non-overlapping HCP subsets\n",
    "shuffled_hcp = random.sample(hcp_pool, n_augmentations * subset_size)\n",
    "\n",
    "hcp_subsets = [\n",
    "    shuffled_hcp[i * subset_size : (i + 1) * subset_size]\n",
    "    for i in range(n_augmentations)\n",
    "]"
   ],
   "id": "40e800dc35c89f40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each subset has exactly 17 elements",
   "id": "64269fa076d9f986"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for idx, subset in enumerate(hcp_subsets, start=1):\n",
    "    if len(subset) != subset_size:\n",
    "        print(f\"Subset {idx} has incorrect length: {len(subset)} elements\")\n",
    "    else:\n",
    "        print(f\"Subset {idx} OK ({len(subset)} HCP)\")"
   ],
   "id": "2d82b72b387924ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check which HCPs were left out",
   "id": "a5d1273ef81fe6cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "used_hcp = set(shuffled_hcp)\n",
    "hcp_pool_set = set(hcp_pool)\n",
    "excluded_hcp = hcp_pool_set - used_hcp\n",
    "\n",
    "print(\"List of excluded HCPs:\", sorted(excluded_hcp))"
   ],
   "id": "909e19bed65c29a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check for overlaps between subsets",
   "id": "37556af58fff5726"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Flatten all subsets into a single list\n",
    "all_hcps = [hcp for subset in hcp_subsets for hcp in subset]\n",
    "\n",
    "# Compare total length vs. number of unique HCPs\n",
    "if len(all_hcps) == len(set(all_hcps)):\n",
    "    print(\"Subsets are disjoint: no overlaps.\")\n",
    "else:\n",
    "    print(\"Warning: there are overlapping HCPs between subsets.\")"
   ],
   "id": "668a91915ff648cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Augmentation Check",
   "id": "a43ab1539ccdd292"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### CSV",
   "id": "eadb802ea2df1662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "aug_track = pd.read_csv(csv_aug_path)",
   "id": "7857afb66a313c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "aug_track.iloc[:5, :5]",
   "id": "56e6259075e01d15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that there are exactly 177 unique subjects",
   "id": "39f65666a9894b77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_subjects = aug_track['subject'].unique()\n",
    "print(f\"Number of unique subjects: {len(unique_subjects)}\")"
   ],
   "id": "994a8e8fe51a7fca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each subject has 10 augmentations",
   "id": "fa1d55ebc23e734d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "augment_per_subject = aug_track.groupby(\"subject\")[\"augmentation\"].count()\n",
    "subjects_missing = augment_per_subject[augment_per_subject != 10]\n",
    "\n",
    "if subjects_missing.empty:\n",
    "    print(\"All subjects have exactly 10 augmentations.\")\n",
    "else:\n",
    "    print(\"Some subjects DO NOT have 10 augmentations\")\n",
    "    print(subjects_missing)"
   ],
   "id": "78ec9fb9b87eb1dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each augmentation has exactly 17 HCPs for all patients",
   "id": "35560748506c6cbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Verify that each row in the DataFrame has exactly 17 HCPs\n",
    "aug_track[\"hcp_count\"] = aug_track[\"hcp_subset\"].apply(lambda x: len(x.split(\",\")))\n",
    "\n",
    "# Filter rows with fewer or more than 17 HCPs\n",
    "invalid_hcp_counts = aug_track[aug_track[\"hcp_count\"] != 17]\n",
    "\n",
    "# Output\n",
    "if invalid_hcp_counts.empty:\n",
    "    print(\"All augmentations have exactly 17 HCPs.\")\n",
    "else:\n",
    "    print(\"Some augmentations do NOT have 17 HCPs\")"
   ],
   "id": "b66f81fc3bd677cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Excluding inconsistent subjects, check that valid ones use the same HCPs per augmentation index across all subjects.\n",
    "That is:\n",
    "- **Sub1**\n",
    "    - *aug 1*: hcp1, hcp2, hcp3\n",
    "    - *aug 2*: hcp4, hcp5, hcp6\n",
    "    -  ...\n",
    "...\n",
    "- **Sub2**\n",
    "    - *aug 1*: hcp1, hcp2, hcp3\n",
    "    - *aug 2*: hcp4, hcp5, hcp6\n",
    "    -  ..."
   ],
   "id": "eca0fe8cd4be52c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Exclude subjects with incomplete augmentations\n",
    "valid_aug_track = aug_track[aug_track[\"hcp_count\"] == 17].copy()\n",
    "\n",
    "# For each augmentation index, compute the most frequent HCP group\n",
    "ref_hcp_by_aug = valid_aug_track.groupby(\"augmentation\")[\"hcp_subset\"].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# Assign expected value to each row\n",
    "valid_aug_track[\"expected_hcp_subset\"] = valid_aug_track[\"augmentation\"].map(ref_hcp_by_aug)\n",
    "\n",
    "# Find inconsistent subjects\n",
    "df_inconsistent = valid_aug_track[valid_aug_track[\"hcp_subset\"] != valid_aug_track[\"expected_hcp_subset\"]]\n",
    "\n",
    "# Final output\n",
    "if df_inconsistent.empty:\n",
    "    print(\"Same HCPs used for each augmentation across all VALID subjects.\")\n",
    "else:\n",
    "    print(\"Some augmentations have different HCPs between subjects (only among valid ones):\")\n",
    "    display(df_inconsistent[[\"subject\", \"augmentation\", \"hcp_subset\", \"expected_hcp_subset\"]].sort_values([\"augmentation\", \"subject\"]))\n"
   ],
   "id": "fda58ef4c05a2536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Subjects with inconsistencies",
   "id": "171bfd71f8fc9e21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Subjects with inconsistencies in HCP subset:\")\n",
    "print(invalid_hcp_counts[\"subject\"].unique())"
   ],
   "id": "e7f6bf6aec1589d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Example: inspect one subject with inconsistencies",
   "id": "10fbf3eec8cb9d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter rows for a specific subject\n",
    "filtered = aug_track[aug_track[\"subject\"] == \"4_S_5005\"]"
   ],
   "id": "40a0dacc2a4d65d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "filtered[[\"subject\", \"augmentation\", \"missing_hcps\"]].sort_values(\"augmentation\").reset_index()",
   "id": "54dc4ce1270b60ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Folder",
   "id": "7d36e37a93085602"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if augmentations were actually created",
   "id": "da379ee499fdc3a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List all subfolders\n",
    "subfolders = [f.path for f in os.scandir(fcmaps_augmented_dir) if f.is_dir()]\n",
    "print(f\"Total subfolders (Patients): {len(subfolders)}\\n\")\n",
    "\n",
    "count_problems = 0\n",
    "# Check each subfolder\n",
    "for folder in subfolders:\n",
    "    # List only files (ignore subdirectories)\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    num_files = len(files)\n",
    "\n",
    "    # Only print if not exactly 10\n",
    "    if num_files != 10:\n",
    "        print(f\"[WARNING] {folder} --> {num_files} files (expected 10)\")\n",
    "        count_problems += 1\n",
    "\n",
    "if count_problems == 0:\n",
    "    print(\"All folders contain exactly 10 files.\")\n",
    "else:\n",
    "    print(f\"Total folders with problems: {count_problems}\")"
   ],
   "id": "e78c57ae1e796432"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check that each subject has 10 unique augmentations",
   "id": "fa6bf15dc4c0e274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "problems = []\n",
    "\n",
    "# Loop over subject folders\n",
    "for subject_dir in os.listdir(fcmaps_augmented_dir):\n",
    "    subject_path = os.path.join(fcmaps_augmented_dir, subject_dir)\n",
    "    if os.path.isdir(subject_path):\n",
    "        files = [f for f in os.listdir(subject_path) if f.endswith(\".nii.gz\")]\n",
    "\n",
    "        # Extract augmentation indices from filenames\n",
    "        found_aug = sorted([\n",
    "            int(f.split(\"aug\")[-1].split(\".\")[0])\n",
    "            for f in files\n",
    "            if \"aug\" in f and f.split(\"aug\")[-1].split(\".\")[0].isdigit()\n",
    "        ])\n",
    "\n",
    "        expected = list(range(1, 11))\n",
    "        if found_aug != expected:\n",
    "            problems.append((subject_dir, found_aug))\n",
    "\n",
    "# Final report\n",
    "if problems:\n",
    "    print(\"Problems found in the following subjects:\")\n",
    "    for subj, augs in problems:\n",
    "        print(f\"[PROBLEM] {subj} has augmentations: {augs}\")\n",
    "else:\n",
    "    print(\"All folders contain augmentations from 1 to 10 correctly.\")\n"
   ],
   "id": "99581328dd2b7734"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing HCPs and SCA files missing",
   "id": "bc4376f8eda88c38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "HCP missing as SCA files in the dataset (Lorenzo folder) and here",
   "id": "1a685d6ade8ee691"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSVs\n",
    "sca_missing_df = pd.read_csv(path_sca_missing)\n",
    "aug_track = pd.read_csv(csv_aug_path)\n",
    "\n",
    "# Variables\n",
    "n_total_hcp = 173\n",
    "results = []\n",
    "unexpected_missing_total = set()\n",
    "\n",
    "# Loop through subjects\n",
    "for subject in sca_missing_df[\"subject\"].unique():\n",
    "    # From missing_SCA_files\n",
    "    row_sca = sca_missing_df[sca_missing_df[\"subject\"] == subject]\n",
    "    sca_missing = set(row_sca.iloc[0][\"SCA_files_missing\"].split(\",\"))\n",
    "    count_missing = len(sca_missing)\n",
    "\n",
    "    # From aug_tracking\n",
    "    sub_aug = aug_track[aug_track[\"subject\"] == subject]\n",
    "    expected_hcp = set()\n",
    "    for row in sub_aug[\"missing_hcps\"]:\n",
    "        hcp_ids = row.strip().strip('\"').split(\",\")\n",
    "        expected_hcp.update(hcp_ids)\n",
    "\n",
    "    # Intersections\n",
    "    expected_and_missing = sca_missing.intersection(expected_hcp)\n",
    "    unexpected_and_missing = sca_missing - expected_hcp\n",
    "    unexpected_missing_total.update(unexpected_and_missing)\n",
    "\n",
    "    # Subject output\n",
    "    results.append(f\"Soggetto {subject}:\\n\")\n",
    "    results.append(f\"- Missing SCA files: {count_missing}\\n\")\n",
    "    results.append(f\"- Used SCA files:  {n_total_hcp - count_missing}\\n\")\n",
    "    results.append(f\"- File attesi nell'augmentation e mancanti come SCA: {', '.join(sorted(expected_and_missing))}\\n\")\n",
    "    results.append(f\"- File NON attesi nell'augmentation e mancanti come SCA: {', '.join(sorted(unexpected_and_missing))}\\n\\n\")\n",
    "\n",
    "# Final section\n",
    "results.append(\"Unique list of NON expected SCA-missing files:\\n\")\n",
    "results.append(f\"{', '.join(sorted(unexpected_missing_total))}\\n\")\n",
    "\n",
    "if unexpected_missing_total == excluded_hcp:\n",
    "    results.append(f\"----> The NON expected and missing SCA files match those excluded from augmentation.\\n\")\n",
    "\n",
    "# Write report to file\n",
    "with open(path_final_report, \"w\") as f:\n",
    "    f.writelines(results)\n"
   ],
   "id": "ccb3ab6b41d6615e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
