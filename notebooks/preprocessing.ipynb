{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utils",
   "id": "a4fb29b5f0834348"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Libraries",
   "id": "9673711cf4d6e5e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import hdbscan\n",
    "import umap\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp"
   ],
   "id": "769169badf656678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "sys.path.append(\"/Users/emmatosato/Documents/PhD/ANM_Verona/src/data_processing\")"
   ],
   "id": "64f70b2dd2e9ae8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import importlib\n",
    "from data_processing import umap_clustering, umap_regression, clustering_evaluation, umap_run, processing_flat\n",
    "\n",
    "importlib.reload(clustering_evaluation)\n",
    "importlib.reload(umap_regression)\n",
    "importlib.reload(umap_clustering)\n",
    "importlib.reload(umap_run)\n",
    "importlib.reload(processing_flat)\n",
    "\n",
    "from data_processing.clustering_evaluation import evaluate_kmeans, evaluate_gmm, evaluate_hdbscan, evaluate_consensus\n",
    "from data_processing.umap_regression import main_regression, plot_ols_diagnostics, remove_missing_values, plot_actual_vs_predicted\n",
    "from data_processing.umap_clustering import plot_clusters_vs_groups\n",
    "from data_processing.umap_run import x_features_return, run_umap"
   ],
   "id": "cccdd649b9073d51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "35696279a36baa14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Paths and folders",
   "id": "762bb46b75f4bdbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# FC maps directories\n",
    "dir_FCmaps = '/Users/emmatosato/Documents/PhD/ANM_Verona/data/FCmaps'\n",
    "dir_FCmaps_processed = '/data/FCmaps_augmented_processed'\n",
    "dir_data_utils = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data_utils/\"\n",
    "\n",
    "# Path to masks\n",
    "gm_mask_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/utils/masks/GM_mask.nii'\n",
    "harvard_oxford_mask_path = '/Users/emmatosato/Documents/PhD/ANM_Verona/utils/masks/mask_GM.nii'\n",
    "\n",
    "# Metadata\n",
    "atrophy_dataset_matches = '/Users/emmatosato/Documents/PhD/ANM_Verona/utils/metadata/atrophy_matched.xlsx'\n",
    "cognitive_dataset ='/Users/emmatosato/Documents/PhD/ANM_Verona/utils/metadata/cognitive_dataset.xlsx'\n",
    "path_df_meta = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data/dataframes/meta/df_meta.csv\"\n",
    "\n",
    "# Done dataframe of preprocessed FC maps\n",
    "path_df_gm = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data/dataframes/fdc/df_gm.pkl\"\n",
    "path_df_thr02_gm = \"/Users/emmatosato/Documents/PhD/ANM_Verona/data/dataframes/fdc/df_thr02_gm.pkl\""
   ],
   "id": "8712d8150a93e04e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data",
   "id": "1fe9fd9849f07fba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Raw data",
   "id": "4286d94b6d93f5cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### One file example",
   "id": "cef343d7b03d08a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load an image\n",
    "path_temp = os.path.join(dir_FCmaps, '1_S_5005.FDC.nii.gz')\n",
    "img = nib.load(path_temp)\n",
    "\n",
    "# Separate the data and affine\n",
    "temp_img = img.get_fdata()      # 3D volume data\n",
    "temp_affine = img.affine        # 4x4 affine matrix\n",
    "\n",
    "# Print shapes\n",
    "print(temp_img.shape)\n",
    "print(temp_affine.shape)"
   ],
   "id": "b20e7f28d74e77fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load data",
   "id": "20ddbad83e889a36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function for listing the data in nii.gz format and extract the subjects identifiers + printing some infos",
   "id": "ed52e15722bfe879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def listData_saveID(dir):\n",
    "    # All nii.gz files in the directory\n",
    "    files_path = sorted(glob.glob(os.path.join(dir, '*gz')))\n",
    "\n",
    "    # Extract Subject IDs from filenames\n",
    "    subject_id = [os.path.basename(f).replace('.FDC.nii.gz', '') for f in files_path]\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(files_path) == len(subject_id), (\n",
    "        f\"Mismatch count: {len(files_path)} files vs {len(subject_id)} IDs\"\n",
    "    )\n",
    "    assert len(subject_id) == len(set(subject_id)), \"ID duplicated\"\n",
    "    for fp, sid in zip(files_path, subject_id):\n",
    "        fname = os.path.basename(fp)\n",
    "        expected = sid + '.FDC.nii.gz'\n",
    "        assert fname == expected, (\n",
    "            f\"Filename “{fname}” do not correspond to the extracted ID“{sid}”\"\n",
    "        )\n",
    "\n",
    "    print(\"Check length:\")\n",
    "    print(\"Files: \", len(files_path))\n",
    "    print(\"Subject: \", len(subject_id))\n",
    "\n",
    "    return files_path, subject_id"
   ],
   "id": "630d3c8e453f4c34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "files, sub_id = listData_saveID(dir_FCmaps)",
   "id": "d82fb5d6d9ffcedd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "maps_FDC = []\n",
    "for path in files:\n",
    "    print(path)\n",
    "    data = nib.load(path).get_fdata().flatten()\n",
    "    maps_FDC.append(data)\n",
    "\n",
    "# Stores subjects as rows and voxels as columns\n",
    "df=pd.DataFrame(maps_FDC)"
   ],
   "id": "74f12fc280995b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Copy without id\n",
    "df_eda = df.copy()"
   ],
   "id": "721e7175cbc40e4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Insert subjects id\n",
    "df.insert(0, 'ID', sub_id)\n",
    "print(\"\\n\", df.shape, \"\\n\")"
   ],
   "id": "96ba4ff3a15fef9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some asserts",
   "id": "22fbcc9b8dfe06d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1Ensure the EDA copy has one row per file\n",
    "assert df_eda.shape[0] == len(files), (\n",
    "    f\"Rows in df_eda ({df_eda.shape[0]}) != number of files ({len(files)})\"\n",
    ")\n",
    "\n",
    "# Ensure the df has one row per subject ID\n",
    "assert df.shape[0] == len(sub_id), (\n",
    "    f\"Number of rows ({df.shape[0]}) != number of IDs ({len(sub_id)})\"\n",
    ")\n",
    "\n",
    "# Verify the ID column matches the original sub_id order exactly\n",
    "assert df['ID'].tolist() == sub_id, \"Row IDs do not match the original sub_id order\"\n",
    "\n",
    "# Confirm there are no duplicate IDs\n",
    "assert df['ID'].is_unique, \"There are duplicate IDs in the DataFrame\"\n",
    "\n",
    "# Check that inserting the ID column did not alter the numeric data\n",
    "numeric_equal = (df.drop('ID', axis=1).values == df_eda.values).all()\n",
    "assert numeric_equal, \"Numeric data was altered when inserting the ID column\""
   ],
   "id": "d0611cfa0379e13e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### EDA",
   "id": "5db42728b19828f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(f\"Shape of the dataframes: {df_eda.shape}\")",
   "id": "42ee9f3e3483c447"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Global Statistics\n",
    "all_voxels = df_eda.values.flatten()\n",
    "all_voxels = all_voxels[~np.isnan(all_voxels)]\n",
    "\n",
    "# Descriptive statistics\n",
    "stats_summary = {\n",
    "    \"Min\": np.min(all_voxels),\n",
    "    \"Max\": np.max(all_voxels),\n",
    "    \"Mean\": np.mean(all_voxels),\n",
    "    \"Std\": np.std(all_voxels),\n",
    "    \"1st Percentile\": np.percentile(all_voxels, 1),\n",
    "    \"25th Percentile\": np.percentile(all_voxels, 25),\n",
    "    \"Median (50th)\": np.percentile(all_voxels, 50),\n",
    "    \"75th Percentile\": np.percentile(all_voxels, 75),\n",
    "    \"99th Percentile\": np.percentile(all_voxels, 99),\n",
    "}\n",
    "\n",
    "df_stats = pd.DataFrame.from_dict(stats_summary, orient='index', columns=['Value'])\n",
    "print(df_stats)"
   ],
   "id": "7adf8e8d33ba7c63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metadata",
   "id": "22f5732cc9ad6be5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dataset containing:\n",
    "- ID of the subject\n",
    "- The diagnosis (Group)\n",
    "- Sex\n",
    "- Age\n",
    "- Education\n",
    "#\n",
    "The regressors:\n",
    "- CDR_SB: disease gravity with a larger range\n",
    "- CDR: same but smaller range"
   ],
   "id": "535cf8684d844cac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the metadata and align to FC map order\n",
    "df_meta = pd.read_excel(cognitive_dataset, sheet_name='Sheet1')\n",
    "df_meta['Age'] = df_meta['Age'].round(1)"
   ],
   "id": "5750221a202ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_meta.head(5)",
   "id": "2db7a7e840be343"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### EDA on Labels",
   "id": "e6ed35cf4d13d1f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Meta dataset BEFORE removing a subject\")\n",
    "print(df_meta.shape)\n",
    "\n",
    "# Remove the subject with ID \"4_S_5003\"\n",
    "df_meta = df_meta[df_meta['ID'] != '4_S_5003'].reset_index(drop=True)\n",
    "\n",
    "print(\"Meta dataset AFTER removing a subject\")\n",
    "print(df_meta.shape)"
   ],
   "id": "9012312b1fbddb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Saving the new csv\n",
    "df_meta.to_csv(\"/Users/emmatosato/Documents/PhD/ANM_Verona/utils/metadata/labels.csv\", index=False)"
   ],
   "id": "7922926ce4d663ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "General info",
   "id": "7db8f2e7c84db4bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(df_meta.info())",
   "id": "6f4b9f8ac57727f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Missing values",
   "id": "48f30eb1aec13604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nMissing values for column:\")\n",
    "print(df_meta.isna().sum())"
   ],
   "id": "fd2979bcdb16b012"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unique values WITH NaN",
   "id": "77e33fd57efbc058"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nUnique Values:\")\n",
    "print(\"Group:\", sorted(df_meta['Group'].unique()))\n",
    "print(\"CDR_SB:\", np.sort(df_meta['CDR_SB'].unique()))\n",
    "print(\"MMSE:\", np.sort(df_meta['MMSE'].unique()))"
   ],
   "id": "49d437848fdd5ae6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unique values WITHOUT NaN",
   "id": "8984503bc137d3a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cdr_sb_no_nan = df_meta['CDR_SB'].dropna()\n",
    "mmse_no_nan = df_meta['MMSE'].dropna()"
   ],
   "id": "5ce16b2f9653fa23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nUnique Values:\")\n",
    "print(\"CDR_SB:\", np.sort(cdr_sb_no_nan.unique()))\n",
    "print(\"MMSE:\", np.sort(mmse_no_nan.unique()))"
   ],
   "id": "afa57f216110e733"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(df_meta.select_dtypes(include='number').describe().round(1))",
   "id": "d75bff81a2b79cf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(mmse_no_nan, bins=25, ax=axes[0], color='#61bdcd', edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title(\"MMSE Distribution\", fontsize=14, weight='bold')\n",
    "\n",
    "sns.histplot(cdr_sb_no_nan, bins=25, ax=axes[1], color='#61bdcd', edgecolor='black', alpha=0.85, kde=True)\n",
    "axes[1].set_title(\"CDR_SB Distribution\", fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e1ce9f1c4ccda4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### GMM on CDR_SB",
   "id": "efeb37054fbadbea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Assigning each CDB_SB value to a cluster using GMM, removing NaN values before\n",
    "- New column for the metadata dataframe"
   ],
   "id": "ef224300878b6fa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter valid CDR_SB values\n",
    "df_cdr = df_meta[['ID', 'CDR_SB']].dropna().copy()\n",
    "print(\"Dimensions after dropping NaN\", df_cdr.shape)\n",
    "\n",
    "# Fit GMM and predict raw labels\n",
    "np.random.seed(42)\n",
    "x_gmm = df_cdr['CDR_SB'].values.reshape(-1, 1)\n",
    "gmm = GaussianMixture(n_components=3, random_state=42).fit(x_gmm)\n",
    "df_cdr['GMM_Label'] = gmm.predict(x_gmm)\n",
    "\n",
    "# Reorder labels by CDR_SB severity\n",
    "means = df_cdr.groupby('GMM_Label')['CDR_SB'].mean().sort_values()\n",
    "label_map = {old: new for new, old in enumerate(means.index)}\n",
    "df_cdr['GMM_Label'] = df_cdr['GMM_Label'].map(label_map)\n",
    "\n",
    "# Include in metadata\n",
    "label_map = dict(zip(df_cdr['ID'], df_cdr['GMM_Label']))\n",
    "df_meta = df_meta.drop(columns=['GMM_Label'], errors='ignore')\n",
    "df_meta['GMM_Label'] = df_meta['ID'].map(label_map).astype('Int64')"
   ],
   "id": "d26eb833d666f81d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(df_meta['GMM_Label'].value_counts().sort_index())",
   "id": "59bc2e5b852d09f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "set_2 = sns.color_palette(\"Set2\")[2:]\n",
    "\n",
    "sns.histplot(\n",
    "    data=df_meta.dropna(subset=['CDR_SB', 'GMM_Label']),\n",
    "    x='CDR_SB',\n",
    "    hue='GMM_Label',\n",
    "    palette=set_2,\n",
    "    multiple='stack',   # oppure 'dodge' per barre affiancate\n",
    "    bins=35,\n",
    "    edgecolor='black',\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "plt.xlabel(\"CDR_SB\", fontsize=12)\n",
    "plt.ylabel(\"Number of Subjects\", fontsize=12)\n",
    "plt.title(\"CDR_SB Distribution by GMM Cluster\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "30bc864e8e648756"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "aic, bic = evaluate_gmm(x_gmm, K_range=range(2, 9), save_path= None, prefix=\"cdr_sb\", plot_flag=True)",
   "id": "989bf1463ebe30c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Yeo Data",
   "id": "3fdf4ef06f24bbac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_networks_no_thr = pd.read_csv(\"/Users/emmatosato/Documents/PhD/ANM_Verona/data/mean_networks/mean_networks_noTHR.csv\")\n",
    "df_networks_thr01 = pd.read_csv(\"/Users/emmatosato/Documents/PhD/ANM_Verona/data/mean_networks/mean_networks_thr01.csv\")\n",
    "df_networks_thr02 = pd.read_csv(\"/Users/emmatosato/Documents/PhD/ANM_Verona/data/mean_networks/mean_networks_thr02.csv\")"
   ],
   "id": "6e981ff2afac375f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_networks_no_thr = df_networks_no_thr.rename(columns={\"CODE\": \"ID\"})\n",
    "df_networks_thr01 = df_networks_thr01.rename(columns={\"CODE\": \"ID\"})\n",
    "df_networks_thr02 = df_networks_thr02.rename(columns={\"CODE\": \"ID\"})"
   ],
   "id": "d7bc3902dc8c0ff8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "features_network = ['VIS_mean', 'VAN_mean', 'DAN_mean', 'DMN_mean', 'LMB_mean', 'FPN_mean', 'SMN_mean', 'SUBCORTICAL_mean']",
   "id": "72db3f8511134a09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis with UMAP",
   "id": "af8b6ccb24a64d2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data",
   "id": "5d93fa8762f6580"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load data",
   "id": "cedb4a7aa22801b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Recall the dataframe with ID and values of the maps",
   "id": "3d4e4b745afc66e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\", df.shape, \"\\n\")\n",
    "df.iloc[:5, :5]"
   ],
   "id": "e5dad60c824cbc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Merging the meta columns with the above dataframe: the order of the subject must be the same of the dataframe of the FC maps",
   "id": "bf0106a4b411a8fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_meta = df_meta.set_index('ID').loc[df['ID']].reset_index()\n",
    "\n",
    "assert all(df['ID'] == df_meta['ID']), \"Mismatch between ID of df and df_meta_ordered\"\n",
    "print(\"The ID are now perfectly aligned\")"
   ],
   "id": "104242d8ed15d758"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing",
   "id": "c4602bb8e093720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.shape",
   "id": "5ca3e1cc612ef4ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Thresholding",
   "id": "9fd3aa29ccf056d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set a threshold for values below 0.1 and 0.2 for defining correlation",
   "id": "38620222b409d412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def apply_threshold(dataframe, threshold):\n",
    "    df_thr = dataframe.copy()\n",
    "    df_thr.iloc[:, 1:] = df_thr.iloc[:, 1:].mask(df_thr.iloc[:, 1:] < threshold, 0)\n",
    "    return df_thr"
   ],
   "id": "4b7a7f653b69dda3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_thr_01 = apply_threshold(df, threshold=0.1)",
   "id": "88da87923ebe9ad2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_thr_02 = apply_threshold(df, threshold=0.2)",
   "id": "9e83a169c2d98ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### GM Mask",
   "id": "94f53fd295e42d2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Binary mask of grey matter using the Harvard-Oxford Atlas\n",
    "#\n",
    "- Keeping the Gray Matter voxels"
   ],
   "id": "a7760dad6e5c8631"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def apply_mask(df_thr, mask_path):\n",
    "    # Load and flatten GM mask\n",
    "    mask = nib.load(mask_path).get_fdata().flatten()\n",
    "    assert mask.shape[0] == df_thr.shape[1] - 1, \"Mask and data length mismatch\"\n",
    "\n",
    "    # Mask\n",
    "    voxel_data = df_thr.iloc[:, 1:]\n",
    "    voxel_data_masked = voxel_data.loc[:, mask != 0]\n",
    "\n",
    "    # Return the masked dataframes\n",
    "    df_masked = pd.concat([df_thr[['ID']], voxel_data_masked], axis=1)\n",
    "    df_masked.columns = ['ID'] + list(range(voxel_data_masked.shape[1]))\n",
    "    return df_masked"
   ],
   "id": "2fc2f7661e1284c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GM masking\n",
    "# 0.1 threshold\n",
    "df_thr01_gm = apply_mask(df_thr_01, gm_mask_path)\n",
    "\n",
    "# 0.2 threshold\n",
    "df_thr02_gm = apply_mask(df_thr_02, gm_mask_path)\n",
    "\n",
    "# Without threshold\n",
    "df_gm = apply_mask(df, gm_mask_path)"
   ],
   "id": "ca1715bc3a79d980"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Harvard masking\n",
    "# 0.1 threshold\n",
    "df_thr01_har = apply_mask(df_thr_01, harvard_oxford_mask_path)\n",
    "\n",
    "# 0.2 threshold\n",
    "df_thr02_har = apply_mask(df_thr_02, harvard_oxford_mask_path)\n",
    "\n",
    "# Without\n",
    "df_har = apply_mask(df, harvard_oxford_mask_path)"
   ],
   "id": "eb91124bf129b9c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### EDA",
   "id": "1287e0e0ce289420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Dimensions of dataframes after gm masking with 2 different thresholds:\")\n",
    "print(\"0.1 threshold\", df_thr01_gm.shape)\n",
    "print(\"0.2 threshold\", df_thr02_gm.shape)\n",
    "\n",
    "print(\"\\nDimensions of dataframes after harvard masking with 2 different thresholds:\")\n",
    "print(\"0.1 threshold\", df_thr01_har.shape)\n",
    "print(\"0.2 threshold\", df_thr02_har.shape)\n",
    "\n",
    "print(\"\\nDimensions of dataframes after gm masking and harvard masking without thresholding:\")\n",
    "print(\"GM Mask\", df_gm.shape)\n",
    "print(\"Harvard Mask\", df_har.shape)"
   ],
   "id": "261f395de887b088"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def summarize_voxel_data(df_masked, threshold=None):\n",
    "    summary = {}\n",
    "    summary['Shape'] = df_masked.shape\n",
    "\n",
    "    if threshold:\n",
    "        has_low = ((df_masked.iloc[:, 1:] > 0) & (df_masked.iloc[:, 1:] < threshold)).any().any()\n",
    "        summary['Valori tra 0 e threshold'] = has_low\n",
    "    else:\n",
    "        summary['Valori tra 0 e threshold'] = 'N/A'\n",
    "\n",
    "    zero_rows = (df_masked.iloc[:, 1:] == 0).all(axis=1).sum()\n",
    "    summary['Zero maps'] = f\"{zero_rows} su {df_masked.shape[0]}\"\n",
    "\n",
    "    voxel_data = df_masked.iloc[:, 1:].values\n",
    "    nonzero_voxels = voxel_data[voxel_data != 0]\n",
    "\n",
    "    summary['All Min'] = voxel_data.min()\n",
    "    summary['All Max'] = voxel_data.max()\n",
    "    summary['All Mean'] = voxel_data.mean()\n",
    "    summary['All Std'] = voxel_data.std()\n",
    "\n",
    "    summary['Nonzero Min'] = nonzero_voxels.min()\n",
    "    summary['Nonzero Max'] = nonzero_voxels.max()\n",
    "    summary['Nonzero Mean'] = nonzero_voxels.mean()\n",
    "    summary['Nonzero Std'] = nonzero_voxels.std()\n",
    "\n",
    "    return summary"
   ],
   "id": "eef2bd4db50e1adb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset\n",
    "dataframes = {\n",
    "    \"thr_01_gm\": (df_thr01_gm, 0.1),\n",
    "    \"thr_02_gm\": (df_thr02_gm, 0.2),\n",
    "    \"thr_01_har\": (df_thr01_har, 0.1),\n",
    "    \"thr_02_har\": (df_thr02_har, 0.2),\n",
    "    \"gm_no_thr\": (df_gm, None),\n",
    "    \"har_no_thr\": (df_har, None)\n",
    "}\n",
    "\n",
    "# Functions\n",
    "results = []\n",
    "\n",
    "for name, (dfm, thr) in dataframes.items():\n",
    "    summary = summarize_voxel_data(dfm, threshold=thr)\n",
    "    summary['Dataset'] = name\n",
    "    results.append(summary)\n",
    "\n",
    "# Summary\n",
    "df_summary = pd.DataFrame(results).set_index('Dataset')\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ],
   "id": "aa9d52707ea11f48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Summary of voxel data:\")\n",
    "display(df_summary)"
   ],
   "id": "9da9b703be7ce05f"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
